<TechnicalOutline>
The biggest thing concern with running a purely in-browser network is the massively parallel, compute and memory intensive tasks that need to happen without disrupting each other. This is the primary reason that things are architected as they are - it’s also 90% of the reason for the stability test, to see how far we can push the properties of the network: nodes, peers, inferences, workers, tokens per second, etc.
There are a number of solutions for this problem, but my method here has been to build a layered system, with separate databases and typespecs for persistence at each layer. Layers communicate with each other through events, and maintain their own internal queues to manage [impedance mismatches] as they happen.
We also make use of [Web Workers] as often as possible - I say this because some critical pieces of infrastructure, like [anything that relies on WebRTC] cannot be moved into workers, and must remain in the main thread. I’m hoping this gets solved at some point. This project has been a good example to me that the browser is accessible as a place to run complex applications, but it’s also had me find the edges where things still need to get better.
Layers also had the benefit of making things easier to modify, and to add new things. In the future - should Rakis become a longer-lived project - it’ll also make it easier to refactor, migrate and strengthen different parts of the system.

## Layer 1: P2P and peering

The first problem is the oldest problem. How do we build a peer to peer system?
NAT Hole punching
Other people who have solved it
General analysis and comparison
The problems of libp2p-js, helia and other issues
WebRTC spec
STUN and TURN servers and boostrapping
Proof of work ratelimiting
If we have incentive system with increasing contributions for reputation, we might not need this
Gossipsub

Solution
Redundancy built in with multiple distribution channels
Which is heavy because we need to carry multiple libraries around
Which means we need efficient deduping
And auth and crypto implemented on our side
We also need some light peer discovery even though this is not as important




## Layer 2: Inference

Inference Requests - from chains and from p2p
Inference workers
Embedding workers
Queue management and model selection
Inference selection [WIP]
Storage


## Layer 3: Coordination

Network events and requests
Commit and reveal
Timeout-based scheduling events
Quorum building

## Layer 4: Consensus
Embeddings and hashing
Computing final consensus
Propagating consensus results

## Layer 5: Persistence
Persistance is implemented through blockchains
Nodes can connect any number of chain identities to their synthientId, which can allow for incentivization in the future.
Nodes on the rakis network are called synthients, after the race of embodied AIs in the Matrix.
Not implemented yet, but the chain contracts are already written for EVM chains. The way this functions is that an on-chain AI contract is called by other smart contracts with the payment and inference parameters.
This AI contract emits an event, which nodes on the Rakis chain listen to. Once the inference is computed, this is pushed back (either through exit nodes or by pushing multiple consensus outputs to the chain and having the contract check for agreement) and the AI contract calls the calling contract with the remaining capital and the result.
This allows for any chain to be connected up, and for blocks to continue on the original chain while the inference is being processed.

Add any additional key objects, interfaces, as well as sections on inference lifecycle, how an inference goes from a chain request all the way down to being completed through the codebase. Add another section listing all the key projects and dependencies we’re built on.
</TechnicalOutline>